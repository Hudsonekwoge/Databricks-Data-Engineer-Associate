{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5095d7f-0553-4857-a21d-8831f3066737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Objectives\n",
    "- Querying data files\n",
    "- Writing to tables\n",
    "- Performing advanced ETL operations\n",
    "- Discover the potential of higher-order functions and user-defined functions (UDFs) in Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d49d8dba-0b0b-4182-a601-4d62034e8564",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Querying Data Files\n",
    "To initiate a file query, we use the SELECT * FROM syntax, followed by the file format and the path to the file. \n",
    "```sql\n",
    "SELECT * FROM file_format.`/path/to/file`\n",
    "```\n",
    "The filepath is specified between **backticks**, to prevent potential syntax errors and ensure the correct interpretation of the path. \n",
    "\n",
    "A filepath in this context can refer to \n",
    "- A single file\n",
    "- A wildcard character to simultaneously read multiple files; or\n",
    "- An entire directory, assuming that all files within that directory adhere to the same format and schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70716425-5a09-4a5a-bde9-5ecfcc8a7b11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can now demonstrate extracting data directly from files using a real-world dataset representing an online school environment. This dataset consists of three tables:\n",
    "- Students\n",
    "- Enrollments\n",
    "- Courses\n",
    "\n",
    "We begin by running a helper notebook, \"School-Setup\", which can be found within the `Include` subfolder. This helper notebook facilitates downloading of the dataset to the Databricks file system and prepares the working environment accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3ad5aa8-87a8-4478-aeaa-12dd3bcfb70b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/School-Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd4963b2-92be-490e-abda-9bb2a3659048",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Querying JSON Format\n",
    "The student data in this dataset is formatted in JSON. The placeholder `dataset_school` referenced in the following query, is a variable defined within our \"School-Setup\" notebook. It points to the location where the dataset files are stored on the filesystem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7f7e449-c289-4a01-8d7e-71399a4a0ef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "files = dbutils.fs.ls(f\"{dataset_school}/students-json\")\n",
    "display(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ab2f114-9485-4c34-b012-8ce2b54062b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The output above shows that there are 6 JSON files in the `students-json` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "294bb48e-d462-44b3-b719-9a5840f14132",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Reading a single data file\n",
    "To read a single JSON file, the `SELECT` statement is used with the syntax `SELECT * FROM json.`, and then the full path for the JSON file is specified between backticks. We use the `dataset.school` placeholder with the `$` character to reference the location where the dataset files are stored. This placeholder is configured in the \"School-Setup\" notebook: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7178ed28-6d89-48ce-8c28-29a42e1ece76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM json.`${dataset.school}/students-json/export_001.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8231bd02-93d7-489b-8ff5-209a4bdab825",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The result displays the extracted student data, including:\n",
    "- Student ID\n",
    "- Email\n",
    "- GPA score\n",
    "- Profile information (in JSON format); and\n",
    "- The last updated timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a618db2-6264-4e55-9df5-4bc4965d882b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Querying multiple files\n",
    "To query multiple files simultaneously, you can use the wildcard character (*) in the path. For example, you can easily query all JSON files starting with the name `export_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "723bcc94-43ee-483d-b5d9-c1232a7b1bb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM json.`${dataset.school}/students-json/export_*.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b6e04e7-ce39-4de3-ac6a-597e37a77c3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Querying an entire directory\n",
    "You can query and entire directory of files, assuming a consistent format and schema across all files in the directory. In the following query, the directory path is specified instead of an individual file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cfe8838-6235-42b3-b65d-48fada578f26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM json.`${dataset.school}/students-json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a470a23a-5988-4075-ad82-f70e601562f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Recording the source file\n",
    "When dealing with multiple files, adding the `input_file_name` function becomes useful. This built-in Spark SQL function records the source data file for each record. This helps in troubleshooting data-related issues by precisely pinpointing their exact source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c57b866c-779b-44a3-b502-88ed98c881bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT *, input_file_name() source_file FROM json.`${dataset.school}/students-json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad53c675-d5ed-42e8-b9c3-b8a9a6aaaf44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The output above shows in addition to the original columns, a new column `source_file`. This column provides supplementary information about the origin of each record in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2866eb18-423c-4ba3-83ac-1ffb27a53d6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Querying Using the text Format\n",
    "When dealing with a variety of text-based files, including formats such as JSON, CSV, TSV, and TXT, Databricks provides the flexibility to handle them using the text format:\n",
    "```sql\n",
    "SELECT * FROM text.`/path/to/file`\n",
    "```\n",
    "This format allows you to extract the data as raw strings, which provide significant advantages in scenarios where input data might be corrupted or contain anomalies. \n",
    "By extracting data as raw strings, you can leverage custom parsing logic to navigate and extract relevant values from the text-based files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "007532c3-9c53-4bc7-afea-309478db7498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM text.`${dataset.school}/students-json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "826dc63b-a4f4-41d2-918e-805707117d98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The output above displays the student data as raw strings. Each line of the file is loaded as a record within a one-string column, `named` value.\n",
    "\n",
    "With this result, you can easily apply custom parsing or transformationt techniques to extract specific fields, correct anomalies, or reformat the data as needed, for subsequent analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fa54070-e0ce-471e-beab-d45d0b092d47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Querying Using binaryFile Format\n",
    "There are scenarios where the binary representation of file content is essential, such as when working with images or unstructured data. In such cases, the `binaryFile` format is suited for this task:\n",
    "```sql\n",
    "SELECT * FROM binaryFile.`/path/sample_image.png`\n",
    "```\n",
    "In the sample query provided, the `binaryFile` format is employed to query an image (`sample_image.png`), allowing you to work directly with the binary representation of the file's content.\n",
    "\n",
    "We can use the `binaryFile` format to extract the raw bytes and some metadata information of the student files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14a17ca4-2794-48a9-b338-880245d0b985",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM binaryFile.`${dataset.school}/students-json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a21d3b4-e061-4005-9fed-c439fe55b865",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The output of the query provides the following details about each source file:\n",
    "- `path` provides the location of the source file on the storage\n",
    "- `modificationTime` gives the last modification time of the file\n",
    "- `length` indicates the size of the file\n",
    "- `content` represents the binary representation of the file\n",
    "\n",
    "So, by using the binaryFile format, you can access both the content and metadata for files, offering a detailed view of your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ade41fb7-4a7c-4716-aa81-0454103044d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Querying Non-Self-Describing Formats\n",
    "The previous querying approach is particularly effective with self-describing formats that possess a well-defined schema, such as JSON and Parquet. By nature, these formats offer a built-in structure that makes it easy to retrieve and interpret data using `SELECT` queries.\n",
    "\n",
    "However, when dealing with non-self-describing formats such as CSV, the `SELECT` statement may not be as informative. Unlike JSON and Parquet, CSV files lack a predefined schema, making the format less suitable for direct querying. In such cases, additional steps, such as defining a schema, may be necessary for effective data extraction and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e580f761-fe80-4ab6-9e48-dfe39c1f2adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM csv.`${dataset.school}/courses-csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b84e0fb9-f152-4daf-a9ba-2f1260f1bfed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As shown from the output above, the query is not well-parsed:\n",
    "- The header row is extracted as a table row; and\n",
    "- All columns are loaded into a single column, `_c0`.\n",
    "This behaviour is explained by the delimiter - the symbol used to separate columns in the file - which, in this case, is a semicolon rather than the standard comma.\n",
    "\n",
    "This issue highlights a challenge with querying files without a well-defined schema, particulary in formats like CSV. In the upcoming sections, we will learn how to address this challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8dceb19-1dff-4d2c-b770-5cc6dfea3052",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Registering Tables from Files with CTAS\n",
    "Using CTAS statements allow you to register tables from files, particularly when dealing with well-defined schema sources like Parquet files. This process is crucial for loading data into a Lakehouse:\n",
    "`CREATE TABLE table_name AS`\n",
    "```sql\n",
    "SELECT * FROM file_format.`/path/to/file`\n",
    "```\n",
    "\n",
    "CTAS statements simplify the process of creating Delta Lake tables by automatically inferring schema information from the query results. This eliminates the need for manual schema declaration. \n",
    "\n",
    "In the following example, we create and populate the student data using a CTAS statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cccc75fd-f6a4-4cce-a246-cf7e042ca530",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DROP TABLE IF EXISTS students;\n",
    "CREATE TABLE students AS SELECT * FROM json.`${dataset.school}/students-json`;\n",
    "DESCRIBE EXTENDED students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66e120ca-0bfa-4d51-9162-babef506ff37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The output above displays the metadata of the `students` table.\n",
    "\n",
    "- The `Provider` value confirms the creation of a Delta Lake table. This means that the CTAS statement has extracted the data from the JSON files and loaded it into the students table in Delta format (i.e., in Parquet data files along with a Delta transaction log).\n",
    "- Additionally, this table is identified as a _managed_ table, as indicated by the `Type` value.\n",
    "- Moreoever, the schema has automatically been inferred from the query results, a feature common to CTAS statements; making CTAS statements a suitable choice for external data ingestion from sources with well-defined schemas, such as Parquet files. \n",
    "\n",
    "However, it is important to note that CTAS statements come with certain limitations. One such limitation is that CTAS statements do not support specifying additional file options. This becomes a challenge when trying to ingest data from CSV files or other formats that require specific configurations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8b0b6a5-bd0f-4073-ab6e-c0f852b9d405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DROP TABLE IF EXISTS courses_unparsed;\n",
    "CREATE TABLE courses_unparsed AS SELECT * FROM csv.`${dataset.school}/courses-csv`;\n",
    "SELECT * FROM courses_unparsed;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b9fe2c1-b72a-426e-9030-a4bcdef2b503",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The output shows that we have successfully created a Delta Lake table; however, the data is not well-parsed. \n",
    "Typically, CSV files have delimiter or encoding options that need to be specified during the data loading process. In response to this requirement, we will now explore an alternative solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fca29a0d-55fd-4749-bc28-ed442444af9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Registering Tables on Foreign Data Sources\n",
    "In scenarios where additional file options are necessary, an alternative solution is to use the regular `CREATE TABLE` statement, but with the `USING` keyword. \n",
    "\n",
    "The `USING` keyword provides increased flexibility by allowing you to specify the type of foreign data source, such as CSV format, as well as any additional file options, such as delimiter and header presence:\n",
    "`CREATE TABLE table_name (col_name1, col_type1,...) USING data_source OPTIONS (key1 = val1, key2 = val2,...) LOCATION path`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c14adf24-fd5f-47a7-b784-af2492f37b7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "It is crucial to note that:\n",
    "- This method creates an external table, serving as a reference to the files without physically moving the data during table creation, to Delta Lake.\n",
    "- Unlike CTAS statements, which automatically infer schema information, creating a table via the `USING` keyword requires you to provide the schema explicitly. Hence, this method offers more control over the schema definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49d88eb4-4379-4a65-9861-d3595a4cd54d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Examples of foreign data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5960fe20-51f7-43a5-85fd-62d1c39c3f7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Example 1: CSV foreign source\n",
    "To deal with CSV files stored in an external location, the followiing example demonstrates the creation of a table using a CSV foreign source:\n",
    "\n",
    "`CREATE TABLE csv_external_table \n",
    "  (col_name1, col_type1, ...)\n",
    "  USING CSV\n",
    "  OPTIONS (header = \"true\", delimiter = \";\")\n",
    "  LOCATION '/path/to/csv/files'`\n",
    "\n",
    "This code sample:\n",
    "- Creates an external table that points to CSV files located in the specified path.\n",
    "- It configures the `header` option to indicate the presence of a header in the files.\n",
    "- The delimiter option is set to use a semicolon instead of the default comma separator.\n",
    "\n",
    "Let's apply this method to our courses data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3acd0cae-fe4c-44b2-b923-6be03296c7ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE IF NOT EXISTS courses_csv \n",
    "( course_id STRING, title STRING, instructor STRING, category STRING, price DOUBLE)\n",
    "USING CSV \n",
    "OPTIONS (header = \"True\", delimiter = \";\")\n",
    "LOCATION \"${dataset.school}/courses-csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd9cf760-7d62-479a-8f35-4e9cfc8b842c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this example, the courses_csv table is created by \n",
    "- Specifying the CSV format as a foreign source, \n",
    "- Indicating the presence of a header in the files, \n",
    "- Defining the semicolon as the delimiter, and lastly, \n",
    "- Specifying the location of the source files.\n",
    "\n",
    "Once the table is created, querying it shows that we have the courses' data in a well-defined structured form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75e2fba4-c4ea-42c5-8ea8-201cf8640a55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM courses_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13d22f9a-a2e3-4410-87e9-7868c95551ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**NOTE**\n",
    "When working with CSV files as a data source, maintaining the column order becomes crucial, especially if additional data files will be added to the source directory. \n",
    "- Spark relies on the specified order during table creation to load data and apply column names and data types correctly from CSV files. Any changes to the column order could impact the integrity of the data loading process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8eb38a51-8bfc-4353-a67e-e28866eab00c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Example 2: Databases as foreign data sources\n",
    "Another scenario where the `CREATE TABLE` statement with the `USING` keyword proves useful is when creating a table using a JDBC connection, which references data in an external SQL database. \n",
    "- This approach enables you to establish a connection to an external database by defining necessary options such as the _connection string_, `username`, _password_, and the specific _database table_ containing the data.\n",
    "\n",
    "Here's an example of creating an external table using a JDBC connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31e58f03-a93a-4604-9703-2037b30b39bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "/*\n",
    "CREATE TABLE jdbc_external_table \n",
    "USING JDBC\n",
    "OPTIONS (\n",
    "  url = 'jdbc:mysql://your_database_server:port',\n",
    "  dbtable = 'your_table',\n",
    "  user = 'your_username',\n",
    "  password = 'your_password'\n",
    ");\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f070d1b6-399c-402c-96f9-328b8f7f2afc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the example above, the following apply:\n",
    "- The `url` option specifies the JDBC connection string to your external database.\n",
    "- The `dbtable` option indicates the specific table within the external database.\n",
    "- The `user` and `password` are credentials required for authentication.\n",
    "\n",
    "By creating an external table using a JDBC connection, you can access and query data from the external database without physically moving or duplicating the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe5a883b-30e7-4a4b-baba-d84360049b02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### A limitation of using a foreign data source\n",
    "Tables having foreign data sources are not Delta tables. Performance benefits and features offered by Delta Lake, such as time travel and guaranteed access to the most recent version of the data, are not available for these tables. This limitation becomes especially noticeable when dealing with large database tables, potentially leading to performance issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27d24c4d-f24b-4105-8b58-36349c68e80e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE EXTENDED courses_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70b607b6-2e05-4aaf-a6be-a6e89ad35684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The output reveals that\n",
    "- The table is an external table, and that it is not a Delta table, as indicated in the `Provider` value. This means that no data conversion to the Delta format occurred during table creation; instead, the table simply points to the CSV files stored in the external location.\n",
    "- Additionally, the `Storage Properties` value captures all metadata and options specified during table creation, ensuring that data in the location is always read with these specified options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "421c0006-9b05-4397-9165-6032cf698555",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Impact of not having a Delta table\n",
    "Unlike Delta Lake tables which guarantee querying the most recent version of source data, tables registered against other data sources, like CSV, may represent outdated cached data. \n",
    "\n",
    "To illustrate this, we will add new data and observe the resulting behaviour of the table. \n",
    "First, let's check the number of files in the `courses` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f9018ff-4d91-41e5-966d-cc015520712a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "files = dbutils.fs.ls(f\"{dataset_school}/courses-csv\")\n",
    "display(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4093473b-34e0-4222-9d90-796ba91584e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The directory currently contains 4 files. As seen in the following output, each file contains three records; and hence, the table holds 12 records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78823785-c798-4e8c-9b68-1e5010650066",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT COUNT(*) FROM courses_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0796c09-97e7-4005-9e40-b02477fab0a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now, let's run the following Python command to duplicate and rename one of these files. This action simulates the ingestion of new CSV files by a source system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9465c8f-115a-4fc4-8153-dd1f8427a048",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "dbutils.fs.cp(f\"{dataset_school}/courses-csv/export_001.csv\", f\"{dataset_school}/courses-csv/copy_001.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bc37274-8739-4fdc-9a7e-b236c9884537",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "After this operation, exploring the courses directory confirms that the new file has been added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fe3831f-09a4-4181-855d-aafcd1a1c3d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "files = dbutils.fs.ls(f\"{dataset_school}/courses-csv\")\n",
    "display(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "806eceaa-e586-4db0-864a-e13446051f0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Despite adding new data to the directory, we notice that the table does not immediately reflect the changes from 12 to 15 records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f927154-0988-4b30-97a6-2565972ec6e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT COUNT(*) FROM courses_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8354b88-0732-4d41-aed5-c611c516ac55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Spark automatically caches the underlying data in local storage for better performance in subsequent queries. However, the external CSV file does not natively signal Spark to refresh this cached data. Consequently, the new data remains invisible until the cache is manually refreshed using the `REFRESH TABLE` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2329eb3b-c75b-4368-a47d-616a674a8f74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "REFRESH TABLE courses_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad82905e-b51f-44b7-b34e-eada03d4b7a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "However, the action invalidates the table cache, necessitating a rescan of the original data source to reload all data into memory. This process can be particularly time-consuming when dealing with large datasets. \n",
    "\n",
    "Upon refreshing the table, querying it again retrieves the updated count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdcddf52-e268-49ea-ac34-d5685634c772",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT COUNT(*) FROM courses_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e0c2ba2-0840-479f-ac86-5a3eec92303e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This observation emphasises the trade-offs and considerations associated with choosing between Delta tables and foreign data sources when working with Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e2355bf-0900-419f-bbfd-e23d4fe1c379",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### A hybrid approach\n",
    "To address this limitation and leverage the advantages of Delta Lake, a workaround involves:\n",
    "- Creating a temporary view that refers to the foreign data source, then\n",
    "- Executing a CTAS statement on this temporary view to extract data from the external source and load it into a Delta table.\n",
    "\n",
    "Here's an illustrative example of this process:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d078ce0-34c5-45c9-9f6c-561edd99d12c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "/*\n",
    "CREATE TEMP VIEW foreign_source_tmp_vw (\n",
    "  col1 col1_type, ...\n",
    ")\n",
    "  USING data_source\n",
    "  OPTIONS (key1 = \"val1\", key2 = \"val2\", ..., path = \"/path/to/data\");\n",
    "\n",
    "CREATE TABLE AS delta_table AS SELECT * FROM foreign_source_tmp_vw;\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b7796ab-738e-4fe4-8571-6eab0880406d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the example above, a temporary view is created referring to a foreign data source. A Delta Lake table is then created by executing a CTAS statement on the temporary view. This process moves the data into a Delta format (Parquet data files + transaction log in JSON format)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f356ed5f-ab5e-4126-89a5-869306a3134b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the same way, we can apply this approach on the course data, delivered in CSV format. \n",
    "- We first create a temporary view and configure it to handle file options. Then,\n",
    "- We execute a CTAS statement to make a copy of the data from the temporary view into a Delta Lake table, named `courses`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9072ffce-f8cc-45b5-8839-8d468a7d7949",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "CREATE TEMP VIEW courses_tmp_vw (course_id STRING, title STRING, instructor STRING, category STRING, price DOUBLE)\n",
    "USING CSV\n",
    "OPTIONS (\n",
    "  path \"${dataset.school}/courses-csv/export_*.csv\",\n",
    "  header = \"true\",\n",
    "  delimiter = \";\"\n",
    ");\n",
    "\n",
    "\n",
    "CREATE TABLE courses AS SELECT * FROM courses_tmp_vw; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "947c7b6a-db68-4874-8525-7ecda3e8d6ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The output below displays the metadata information of the courses table. It confirms that it is a Delta Lake table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5fcc473-0ed2-4eab-8ae9-6fee292f8066",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE EXTENDED courses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d4ac991-c40d-41dc-bce8-7f78cd6211be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Finally, querying the table confirms that it contains well-parsed data from the CSV files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "333c4e8c-6f3a-4da9-8780-fb6da40f3473",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752580375115}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84410498-6b53-49fb-938c-6f59809b15d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7529856598796892,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Module 2: Transforming Data with Apache Spark",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
