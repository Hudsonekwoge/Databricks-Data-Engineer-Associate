{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5095d7f-0553-4857-a21d-8831f3066737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Objectives\n",
    "- Querying data files\n",
    "- Writing to tables\n",
    "- Performing advanced ETL operations\n",
    "- Discover the potential of higher-order functions and user-defined functions (UDFs) in Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d49d8dba-0b0b-4182-a601-4d62034e8564",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Querying Data Files\n",
    "To initiate a file query, we use the SELECT * FROM syntax, followed by the file format and the path to the file. \n",
    "```sql\n",
    "SELECT * FROM file_format.`/path/to/file`\n",
    "```\n",
    "The filepath is specified between **backticks**, to prevent potential syntax errors and ensure the correct interpretation of the path. \n",
    "\n",
    "A filepath in this context can refer to \n",
    "- A single file\n",
    "- A wildcard character to simultaneously read multiple files; or\n",
    "- An entire directory, assuming that all files within that directory adhere to the same format and schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70716425-5a09-4a5a-bde9-5ecfcc8a7b11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can now demonstrate extracting data directly from files using a real-world dataset representing an online school environment. This dataset consists of three tables:\n",
    "- Students\n",
    "- Enrollments\n",
    "- Courses\n",
    "\n",
    "We begin by running a helper notebook, \"School-Setup\", which can be found within the `Include` subfolder. This helper notebook facilitates downloading of the dataset to the Databricks file system and prepares the working environment accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3ad5aa8-87a8-4478-aeaa-12dd3bcfb70b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/School-Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd4963b2-92be-490e-abda-9bb2a3659048",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Querying JSON Format\n",
    "The student data in this dataset is formatted in JSON. The placeholder `dataset_school` referenced in the following query, is a variable defined within our \"School-Setup\" notebook. It points to the location where the dataset files are stored on the filesystem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7f7e449-c289-4a01-8d7e-71399a4a0ef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "files = dbutils.fs.ls(f\"{dataset_school}/students-json\")\n",
    "display(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ab2f114-9485-4c34-b012-8ce2b54062b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The output above shows that there are 6 JSON files in the `students-json` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "294bb48e-d462-44b3-b719-9a5840f14132",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Reading a single data file\n",
    "To read a single JSON file, the `SELECT` statement is used with the syntax `SELECT * FROM json.`, and then the full path for the JSON file is specified between backticks. We use the `dataset.school` placeholder with the `$` character to reference the location where the dataset files are stored. This placeholder is configured in the \"School-Setup\" notebook: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7178ed28-6d89-48ce-8c28-29a42e1ece76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM json.`${dataset.school}/students-json/export_001.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8231bd02-93d7-489b-8ff5-209a4bdab825",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The result displays the extracted student data, including:\n",
    "- Student ID\n",
    "- Email\n",
    "- GPA score\n",
    "- Profile information (in JSON format); and\n",
    "- The last updated timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a618db2-6264-4e55-9df5-4bc4965d882b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Querying multiple files\n",
    "To query multiple files simultaneously, you can use the wildcard character (*) in the path. For example, you can easily query all JSON files starting with the name `export_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "723bcc94-43ee-483d-b5d9-c1232a7b1bb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM json.`${dataset.school}/students-json/export_*.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b6e04e7-ce39-4de3-ac6a-597e37a77c3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Querying an entire directory\n",
    "You can query and entire directory of files, assuming a consistent format and schema across all files in the directory. In the following query, the directory path is specified instead of an individual file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cfe8838-6235-42b3-b65d-48fada578f26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM json.`${dataset.school}/students-json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a470a23a-5988-4075-ad82-f70e601562f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Recording the source file\n",
    "When dealing with multiple files, adding the `input_file_name` function becomes useful. This built-in Spark SQL function records the source data file for each record. This helps in troubleshooting data-related issues by precisely pinpointing their exact source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c57b866c-779b-44a3-b502-88ed98c881bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT *, input_file_name() source_file FROM json.`${dataset.school}/students-json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad53c675-d5ed-42e8-b9c3-b8a9a6aaaf44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The output above shows in addition to the original columns, a new column `source_file`. This column provides supplementary information about the origin of each record in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2866eb18-423c-4ba3-83ac-1ffb27a53d6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Querying Using the text Format\n",
    "When dealing with a variety of text-based files, including formats such as JSON, CSV, TSV, and TXT, Databricks provides the flexibility to handle them using the text format:\n",
    "```sql\n",
    "SELECT * FROM text.`/path/to/file`\n",
    "```\n",
    "This format allows you to extract the data as raw strings, which provide significant advantages in scenarios where input data might be corrupted or contain anomalies. \n",
    "By extracting data as raw strings, you can leverage custom parsing logic to navigate and extract relevant values from the text-based files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "007532c3-9c53-4bc7-afea-309478db7498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM text.`${dataset.school}/students-json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "826dc63b-a4f4-41d2-918e-805707117d98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The output above displays the student data as raw strings. Each line of the file is loaded as a record within a one-string column, `named` value.\n",
    "\n",
    "With this result, you can easily apply custom parsing or transformationt techniques to extract specific fields, correct anomalies, or reformat the data as needed, for subsequent analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fa54070-e0ce-471e-beab-d45d0b092d47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Querying Using binaryFile Format\n",
    "There are scenarios where the binary representation of file content is essential, such as when working with images or unstructured data. In such cases, the `binaryFile` format is suited for this task:\n",
    "```sql\n",
    "SELECT * FROM binaryFile.`/path/sample_image.png`\n",
    "```\n",
    "In the sample query provided, the `binaryFile` format is employed to query an image (`sample_image.png`), allowing you to work directly with the binary representation of the file's content.\n",
    "\n",
    "We can use the `binaryFile` format to extract the raw bytes and some metadata information of the student files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14a17ca4-2794-48a9-b338-880245d0b985",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM binaryFile.`${dataset.school}/students-json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a21d3b4-e061-4005-9fed-c439fe55b865",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The output of the query provides the following details about each source file:\n",
    "- `path` provides the location of the source file on the storage\n",
    "- `modificationTime` gives the last modification time of the file\n",
    "- `length` indicates the size of the file\n",
    "- `content` represents the binary representation of the file\n",
    "\n",
    "So, by using the binaryFile format, you can access both the content and metadata for files, offering a detailed view of your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ade41fb7-4a7c-4716-aa81-0454103044d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Querying Non-Self-Describing Formats\n",
    "The previous querying approach is particularly effective with self-describing formats that possess a well-defined schema, such as JSON and Parquet. By nature, these formats offer a built-in structure that makes it easy to retrieve and interpret data using `SELECT` queries.\n",
    "\n",
    "However, when dealing with non-self-describing formats such as CSV, the `SELECT` statement may not be as informative. Unlike JSON and Parquet, CSV files lack a predefined schema, making the format less suitable for direct querying. In such cases, additional steps, such as defining a schema, may be necessary for effective data extraction and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e580f761-fe80-4ab6-9e48-dfe39c1f2adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM csv.`${dataset.school}/courses-csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b84e0fb9-f152-4daf-a9ba-2f1260f1bfed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As shown from the output above, the query is not well-parsed:\n",
    "- The header row is extracted as a table row; and\n",
    "- All columns are loaded into a single column, `_c0`.\n",
    "This behaviour is explained by the delimiter - the symbol used to separate columns in the file - which, in this case, is a semicolon rather than the standard comma.\n",
    "\n",
    "This issue highlights a challenge with querying files without a well-defined schema, particulary in formats like CSV. In the upcoming sections, we will learn how to address this challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8dceb19-1dff-4d2c-b770-5cc6dfea3052",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Registering Tables from Files with CTAS\n",
    "Using CTAS statements allow you to register tables from files, particularly when dealing with well-defined schema sources like Parquet files. This process is crucial for loading data into a Lakehouse:\n",
    "`CREATE TABLE table_name AS`\n",
    "```sql\n",
    "SELECT * FROM file_format.`/path/to/file`\n",
    "```\n",
    "\n",
    "CTAS statements simplify the process of creating Delta Lake tables by automatically inferring schema information from the query results. This eliminates the need for manual schema declaration. \n",
    "\n",
    "In the following example, we create and populate the student data using a CTAS statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cccc75fd-f6a4-4cce-a246-cf7e042ca530",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DROP TABLE IF EXISTS students;\n",
    "CREATE TABLE students AS SELECT * FROM json.`${dataset.school}/students-json`;\n",
    "DESCRIBE EXTENDED students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66e120ca-0bfa-4d51-9162-babef506ff37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The output above displays the metadata of the `students` table.\n",
    "\n",
    "- The `Provider` value confirms the creation of a Delta Lake table. This means that the CTAS statement has extracted the data from the JSON files and loaded it into the students table in Delta format (i.e., in Parquet data files along with a Delta transaction log).\n",
    "- Additionally, this table is identified as a _managed_ table, as indicated by the `Type` value.\n",
    "- Moreoever, the schema has automatically been inferred from the query results, a feature common to CTAS statements; making CTAS statements a suitable choice for external data ingestion from sources with well-defined schemas, such as Parquet files. \n",
    "\n",
    "However, it is important to note that CTAS statements come with certain limitations. One such limitation is that CTAS statements do not support specifying additional file options. This becomes a challenge when trying to ingest data from CSV files or other formats that require specific configurations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8b0b6a5-bd0f-4073-ab6e-c0f852b9d405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DROP TABLE IF EXISTS courses_unparsed;\n",
    "CREATE TABLE courses_unparsed AS SELECT * FROM csv.`${dataset.school}/courses-csv`;\n",
    "SELECT * FROM courses_unparsed;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b9fe2c1-b72a-426e-9030-a4bcdef2b503",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The output shows that we have successfully created a Delta Lake table; however, the data is not well-parsed. \n",
    "Typically, CSV files have delimiter or encoding options that need to be specified during the data loading process. In response to this requirement, we will now explore an alternative solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fca29a0d-55fd-4749-bc28-ed442444af9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Registering Tables on Foreign Data Sources\n",
    "In scenarios where additional file options are necessary, an alternative solution is to use the regular `CREATE TABLE` statement, but with the `USING` keyword. \n",
    "\n",
    "The `USING` keyword provides increased flexibility by allowing you to specify the type of foreign data source, such as CSV format, as well as any additional file options, such as delimiter and header presence:\n",
    "`CREATE TABLE table_name (col_name1, col_type1,...) USING data_source OPTIONS (key1 = val1, key2 = val2,...) LOCATION path`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c14adf24-fd5f-47a7-b784-af2492f37b7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "It is crucial to note that:\n",
    "- This method creates an external table, serving as a reference to the files without physically moving the data during table creation, to Delta Lake.\n",
    "- Unlike CTAS statements, which automatically infer schema information, creating a table via the `USING` keyword requires you to provide the schema explicitly. Hence, this method offers more control over the schema definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49d88eb4-4379-4a65-9861-d3595a4cd54d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Examples of foreign data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5960fe20-51f7-43a5-85fd-62d1c39c3f7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Example 1: CSV foreign source\n",
    "To deal with CSV files stored in an external location, the followiing example demonstrates the creation of a table using a CSV foreign source:\n",
    "\n",
    "`CREATE TABLE csv_external_table \n",
    "  (col_name1, col_type1, ...)\n",
    "  USING CSV\n",
    "  OPTIONS (header = \"true\", delimiter = \";\")\n",
    "  LOCATION '/path/to/csv/files'`\n",
    "\n",
    "This code sample:\n",
    "- Creates an external table that points to CSV files located in the specified path.\n",
    "- It configures the `header` option to indicate the presence of a header in the files.\n",
    "- The delimiter option is set to use a semicolon instead of the default comma separator.\n",
    "\n",
    "Let's apply this method to our courses data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3acd0cae-fe4c-44b2-b923-6be03296c7ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE IF NOT EXISTS courses_csv \n",
    "( course_id STRING, title STRING, instructor STRING, category STRING, price DOUBLE)\n",
    "USING CSV \n",
    "OPTIONS (header = \"True\", delimiter = \";\")\n",
    "LOCATION \"${dataset.school}/courses-csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd9cf760-7d62-479a-8f35-4e9cfc8b842c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this example, the courses_csv table is created by \n",
    "- Specifying the CSV format as a foreign source, \n",
    "- Indicating the presence of a header in the files, \n",
    "- Defining the semicolon as the delimiter, and lastly, \n",
    "- Specifying the location of the source files.\n",
    "\n",
    "Once the table is created, querying it shows that we have the courses' data in a well-defined structured form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75e2fba4-c4ea-42c5-8ea8-201cf8640a55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM courses_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13d22f9a-a2e3-4410-87e9-7868c95551ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**NOTE**\n",
    "When working with CSV files as a data source, maintaining the column order becomes crucial, especially if additional data files will be added to the source directory. \n",
    "- Spark relies on the specified order during table creation to load data and apply column names and data types correctly from CSV files. Any changes to the column order could impact the integrity of the data loading process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8eb38a51-8bfc-4353-a67e-e28866eab00c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Example 2: Databases as foreign data sources\n",
    "Another scenario where the `CREATE TABLE` statement with the `USING` keyword proves useful is when creating a table using a JDBC connection, which references data in an external SQL database. \n",
    "- This approach enables you to establish a connection to an external database by defining necessary options such as the _connection string_, `username`, _password_, and the specific _database table_ containing the data.\n",
    "\n",
    "Here's an example of creating an external table using a JDBC connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31e58f03-a93a-4604-9703-2037b30b39bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "/*\n",
    "CREATE TABLE jdbc_external_table \n",
    "USING JDBC\n",
    "OPTIONS (\n",
    "  url = 'jdbc:mysql://your_database_server:port',\n",
    "  dbtable = 'your_table',\n",
    "  user = 'your_username',\n",
    "  password = 'your_password'\n",
    ");\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f070d1b6-399c-402c-96f9-328b8f7f2afc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the example above, the following apply:\n",
    "- The `url` option specifies the JDBC connection string to your external database.\n",
    "- The `dbtable` option indicates the specific table within the external database.\n",
    "- The `user` and `password` are credentials required for authentication.\n",
    "\n",
    "By creating an external table using a JDBC connection, you can access and query data from the external database without physically moving or duplicating the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe5a883b-30e7-4a4b-baba-d84360049b02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### A limitation of using a foreign data source\n",
    "Tables having foreign data sources are not Delta tables. Performance benefits and features offered by Delta Lake, such as time travel and guaranteed access to the most recent version of the data, are not available for these tables. This limitation becomes especially noticeable when dealing with large database tables, potentially leading to performance issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27d24c4d-f24b-4105-8b58-36349c68e80e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE EXTENDED courses_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70b607b6-2e05-4aaf-a6be-a6e89ad35684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The output reveals that\n",
    "- The table is an external table, and that it is not a Delta table, as indicated in the `Provider` value. This means that no data conversion to the Delta format occurred during table creation; instead, the table simply points to the CSV files stored in the external location.\n",
    "- Additionally, the `Storage Properties` value captures all metadata and options specified during table creation, ensuring that data in the location is always read with these specified options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "421c0006-9b05-4397-9165-6032cf698555",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Impact of not having a Delta table\n",
    "Unlike Delta Lake tables which guarantee querying the most recent version of source data, tables registered against other data sources, like CSV, may represent outdated cached data. \n",
    "\n",
    "To illustrate this, we will add new data and observe the resulting behaviour of the table. \n",
    "First, let's check the number of files in the `courses` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f9018ff-4d91-41e5-966d-cc015520712a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "files = dbutils.fs.ls(f\"{dataset_school}/courses-csv\")\n",
    "display(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4093473b-34e0-4222-9d90-796ba91584e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The directory currently contains 4 files. As seen in the following output, each file contains three records; and hence, the table holds 12 records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78823785-c798-4e8c-9b68-1e5010650066",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT COUNT(*) FROM courses_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0796c09-97e7-4005-9e40-b02477fab0a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now, let's run the following Python command to duplicate and rename one of these files. This action simulates the ingestion of new CSV files by a source system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9465c8f-115a-4fc4-8153-dd1f8427a048",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "dbutils.fs.cp(f\"{dataset_school}/courses-csv/export_001.csv\", f\"{dataset_school}/courses-csv/copy_001.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bc37274-8739-4fdc-9a7e-b236c9884537",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "After this operation, exploring the courses directory confirms that the new file has been added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fe3831f-09a4-4181-855d-aafcd1a1c3d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "files = dbutils.fs.ls(f\"{dataset_school}/courses-csv\")\n",
    "display(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "806eceaa-e586-4db0-864a-e13446051f0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Despite adding new data to the directory, we notice that the table does not immediately reflect the changes from 12 to 15 records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f927154-0988-4b30-97a6-2565972ec6e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT COUNT(*) FROM courses_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8354b88-0732-4d41-aed5-c611c516ac55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Spark automatically caches the underlying data in local storage for better performance in subsequent queries. However, the external CSV file does not natively signal Spark to refresh this cached data. Consequently, the new data remains invisible until the cache is manually refreshed using the `REFRESH TABLE` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2329eb3b-c75b-4368-a47d-616a674a8f74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "REFRESH TABLE courses_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad82905e-b51f-44b7-b34e-eada03d4b7a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "However, the action invalidates the table cache, necessitating a rescan of the original data source to reload all data into memory. This process can be particularly time-consuming when dealing with large datasets. \n",
    "\n",
    "Upon refreshing the table, querying it again retrieves the updated count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdcddf52-e268-49ea-ac34-d5685634c772",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT COUNT(*) FROM courses_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e0c2ba2-0840-479f-ac86-5a3eec92303e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This observation emphasises the trade-offs and considerations associated with choosing between Delta tables and foreign data sources when working with Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e2355bf-0900-419f-bbfd-e23d4fe1c379",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### A hybrid approach\n",
    "To address this limitation and leverage the advantages of Delta Lake, a workaround involves:\n",
    "- Creating a temporary view that refers to the foreign data source, then\n",
    "- Executing a CTAS statement on this temporary view to extract data from the external source and load it into a Delta table.\n",
    "\n",
    "Here's an illustrative example of this process:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d078ce0-34c5-45c9-9f6c-561edd99d12c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "/*\n",
    "CREATE TEMP VIEW foreign_source_tmp_vw (\n",
    "  col1 col1_type, ...\n",
    ")\n",
    "  USING data_source\n",
    "  OPTIONS (key1 = \"val1\", key2 = \"val2\", ..., path = \"/path/to/data\");\n",
    "\n",
    "CREATE TABLE AS delta_table AS SELECT * FROM foreign_source_tmp_vw;\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b7796ab-738e-4fe4-8571-6eab0880406d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the example above, a temporary view is created referring to a foreign data source. A Delta Lake table is then created by executing a CTAS statement on the temporary view. This process moves the data into a Delta format (Parquet data files + transaction log in JSON format)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f356ed5f-ab5e-4126-89a5-869306a3134b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the same way, we can apply this approach on the course data, delivered in CSV format. \n",
    "- We first create a temporary view and configure it to handle file options. Then,\n",
    "- We execute a CTAS statement to make a copy of the data from the temporary view into a Delta Lake table, named `courses`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9072ffce-f8cc-45b5-8839-8d468a7d7949",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "CREATE TEMP VIEW courses_tmp_vw (course_id STRING, title STRING, instructor STRING, category STRING, price DOUBLE)\n",
    "USING CSV\n",
    "OPTIONS (\n",
    "  path \"${dataset.school}/courses-csv/export_*.csv\",\n",
    "  header = \"true\",\n",
    "  delimiter = \";\"\n",
    ");\n",
    "\n",
    "\n",
    "CREATE TABLE courses AS SELECT * FROM courses_tmp_vw; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "947c7b6a-db68-4874-8525-7ecda3e8d6ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The output below displays the metadata information of the courses table. It confirms that it is a Delta Lake table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5fcc473-0ed2-4eab-8ae9-6fee292f8066",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE EXTENDED courses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d4ac991-c40d-41dc-bce8-7f78cd6211be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Finally, querying the table confirms that it contains well-parsed data from the CSV files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "333c4e8c-6f3a-4da9-8780-fb6da40f3473",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752580375115}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM courses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84410498-6b53-49fb-938c-6f59809b15d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Writing to Tables\n",
    "We initiate our exploration by using a CTAS statement to create the enrollments Delta table from Parquet files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a89e321-bb90-4446-ab0e-dfdd8c6642e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/School-Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccfaa97f-18ab-427e-9b26-fba111bdff5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE enrollments AS SELECT * FROM parquet.`${dataset.school}/enrollments`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e86c5ee4-fda9-409e-9a24-657fff041911",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Once the table is created, we proceed to query its content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "747b0e68-25f1-4756-a3fa-377a40158d1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM enrollments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7361a224-3304-4d6e-a863-f397dd593eb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Since Parquet files have a well-defined schema, we observe that Delta Lake has accurately captured the schema and successfully extracted the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b1ffeb6-d8cf-4352-aa2e-25f57d538247",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Replacing Data\n",
    "You can completely replace the content of a Delta Lake table either by\n",
    "- Overwriting the existing table, or by\n",
    "- Other traditional methods like dumping and re-creating it.\n",
    "However, overwriting Delta tables offers several advantages over the approach of merely dropping and re-creating tables:\n",
    "\n",
    "| Category                        | Drop and Recreate Table                                                                 | Overwrite Table                                                                       |\n",
    "|---------------------------------|------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------|\n",
    "| **Processing time**             | Time-consuming as it involves recursively listing directories and deleting large files. | Fast process since the updated data is just a new table version.                      |\n",
    "| **Leveraging Deltaâ€™s time travel** | Deletes the old versions of the table, making its historical data unavailable for retrieval. | Preserves the old table versions, allowing easy retrieval of historical data.         |\n",
    "| **Concurrency**                | Concurrent queries are unable to access the table while the operation is ongoing.       | Concurrent queries can continue reading the table seamlessly while the operation is in progress. |\n",
    "| **ACID guarantees**            | If the operation fails, the table cannot be reverted to its original state.             | If the operation fails, the table will revert to its previous state.                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3dc461f-d827-4d71-b566-c7c90894fbfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In Databricks, there are two methods to completely replace the content of Delta Lake tables:\n",
    "- `CREATE OR REPLACE TABLE` statements\n",
    "- `INSERT OVERWRITE` statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4ca04ae-a7ab-40c0-9f96-e57844f89765",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### CREATE OR REPLACE TABLE statements\n",
    "This is also known as **CRAS** (**CREATE OR REPLACE AS SELECT**) statement. This statement fully replaces the content of a table each time it executes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55cca293-3108-4e2d-8c01-a1ea62208c7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE enrollments AS SELECT * FROM parquet.`${dataset.school}/enrollments`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac76acd0-2246-4d0f-9387-6f24568f2206",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Upon executing this statement, the `enrollments` table will be overwritten with the newer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa61f8e0-1d65-475c-b042-3364ae325cf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE HISTORY enrollments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4de9c18-f5a3-41f6-aa4a-c30d49f80817",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As illustrated in the output, version 0 is a CTAS statement; meanwhile, the CRAS statement has generated a new table version, reflecting the updated state of the table after the overwrite operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42e43703-1503-4de6-bd39-484166654483",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### INSERT OVERWRITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2af9b2fa-669c-414a-8d39-9b95e3fdcbe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "INSERT OVERWRITE enrollments SELECT * FROM parquet.`${dataset.school}/enrollments`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbdb7135-180f-4310-9941-cec2b4e62f10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "While the above statement achieves a similar outcome to the `CREATE OR REPLACE TABLE` approach, there are some key differences and nuances to consider. Unlike the CRAS statement which can create a new table if one does not exist, `INSERT OVERWRITE` can only overwrite an existing table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d25278f-05c3-4ad4-a4a0-5f61571ebd1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE HISTORY enrollments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92a46d80-3ba8-4869-b2bc-c9fa6c23c441",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Delta Lake records the `INSERT OVERWRITE` operation as a standard `WRITE` operation. However, the mode of this operation is marked as `OVERWRITE` in the `operationParameters` field. This indicates that the existing data was replaced with the new records from the query.\n",
    "\n",
    "One significant aspect of `INSERT OVERWRITE` is that it replaces all existing data in the target (table or partition), provided the new data matches the schema. This prevents any risk of accidentally modifying the table structure. Thus, `INSERT OVERWRITE` is considered a more secure approach for overwriting existing tables. \n",
    "\n",
    "When attempting to overwrite data with a schema that differs from the existing schema, a schema mismatch error will be generated. For example, let's consider adding an extra column containing the source file name to our table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e12c0b6-4c7f-476a-b4c9-f0bbf8ea96fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "INSERT OVERWRITE enrollments SELECT *, input_file_name() FROM parquet.`${dataset.school}/enrollments`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "056b4c8a-eae8-4685-acd6-6fe729041c30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The previous command results in an exception indicating a schema mismatch. This occurs because the schema of the new data being inserted does not match the existing schema of the `enrollments` table.\n",
    "**Delta Lake tables are by definition, schema-on-write**, which means Delta Lake enforces schema consistency during write options. Any attempt to write data with a schema that differs from the table's schema will be rejected to maintain data integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "343a92d2-dad6-4fbf-bd3e-4074044469f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Appending Data\n",
    "One of the simplest methods to append records to Delta Lake tables is through the use of the INSERT INTO statement. This statement allows you to easily add new data to existing tables from the result of an SQL query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2118e3ab-6f53-4874-bc44-8e1bce056119",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "INSERT INTO enrollments SELECT * FROM parquet.`${dataset.school}/enrollments-new`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72510ee0-18e0-4662-9d6e-e581ce27be19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "By executing the above `INSERT INTO` statement, we insert 700 new records into our table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09ee5a59-3f77-4a07-ad91-2f7db34775eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT COUNT(1) FROM enrollments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07f5def7-6628-43d5-93d4-975314688fc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "While the `INSERT INTO` statement provides a convenient means of appending records to tables, it lacks built-in mechanisms to prevent the insertion of duplicate data. This means that if the insertion query is executed multiple times, it will write the same records to the target table repeatedly, leading to the creation of duplicate entries. To address this issue effectively, we use an alternative method: the `MERGE INTO` statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d2b7539-1e6e-47cb-b9b0-83e7e5efc550",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Merging Data\n",
    "The MERGE INTO statement enables you to perform **upsert** operations - meaning you can insert new data and update existing records - and even delete records, all within a single statement. Here's the basic syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f36647cd-e3cb-4277-b7f3-2cf3ca9b4661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "/*\n",
    "MERGE INTO target_table AS target\n",
    "USING source_table AS source\n",
    "ON target.id = source.id \n",
    "WHEN MATCHED THEN \n",
    "  UPDATE SET target.id = source.id\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (id, name) VALUES (source.id, source.name)\n",
    "\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f460e1f-1bdc-497e-ad05-d183ff18d6b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Clause             | Purpose                                                               |\n",
    "|--------------------|-----------------------------------------------------------------------|\n",
    "| MERGE INTO         | Target table you want to update or insert into                        |\n",
    "| USING              | Source table or subquery providing new data                           |\n",
    "| ON                 | Match condition between source and target rows                        |\n",
    "| WHEN MATCHED       | What to do when a matching row exists (usually an UPDATE)             |\n",
    "| WHEN NOT MATCHED   | What to do when no matching row exists in target (usually an INSERT)  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c38ac7c-3e0f-4abe-9b88-423794fc9e13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/School-Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09b0df7d-2aa4-49cc-a67d-68ecbb9d5ae1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For example, we can use the `MERGE INTO` statement to update student data with modified email addresses and add new students into the table. \n",
    "\n",
    "To accomplish this, we first create a temporary view containing the updated student data. This will serve as the source from which we will merge changes into our students table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dbf8c5d-2916-46f3-b901-ee8bbd84f26a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMP VIEW students_updates AS SELECT * FROM json.`${dataset.school}/students-json-new`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f79bf766-8e87-4a1a-bc36-4a33a91415b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The following merge operation is executed to merge changes from the `students_updates` temporary view into the target `students` table, using the student ID as the key for matching records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16d88cd0-0713-4542-94c1-43fb41f705e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MERGE INTO students c\n",
    "USING students_updates u\n",
    "ON c.student_id = u.student_id \n",
    "WHEN MATCHED AND c.email IS NULL AND u.email IS NOT NULL THEN\n",
    "  UPDATE SET email = u.email, updated = u.updated \n",
    "WHEN NOT MATCHED THEN INSERT *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b385849b-b959-4927-83dd-48a4ac82a405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Within the above `MERGE INTO` statement, we define two primary actions based on the matching status of records:\n",
    "\n",
    "- **Update action (WHEN MATCHED clause)**: When a match is found between the source and target records, an update action is performed. This action involves updating the email address and the last updated timestamp. We introduce additional conditions: _we check if the email address in the current row is null while the corresponding record in the `students_updates` view contains a valid email address_. For such records, we proceed by updating the email field and the last updated timestamp in the target table.\n",
    "- **Insert action (WHEN NOT MATCHED)**: For records in the `students_updates` view that do not match any existing students based on the student ID, an insertion is triggered. This ensures that all new students are added into our target table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69ec800a-0b8c-4580-aae1-afac5c706214",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Some advantages of using MERGE INTO statements include**:\n",
    "- The ability to execute updates, inserts, and deletes within a single atomic transaction. This ensures data consistency and integrity by treating all operations as a single unit, thereby minimising the risk of inconsitencies or partial changes on the table. \n",
    "- The merge operation serves as an excellent solution for preventing duplicates during record insertion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2758fb1c-4f73-4d14-bebd-e9e5db938dec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's consider another scenario where we have a set of new courses to be inserted, delivered in CSV format. To facilitate this, we'll establish a temporary view based on this new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95254401-95e3-44fc-8541-bc411342e195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMP VIEW courses_updates (course_id STRING, title STRING, instructor STRING, category STRING, price DOUBLE)\n",
    "USING CSV\n",
    "OPTIONS (\n",
    "  path = \"${dataset.school}/courses-csv-new\",\n",
    "  header = \"true\",\n",
    "  delimiter = \";\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "083f3580-c954-4fc0-ae8e-3b312bede233",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now, we can use the `MERGE INTO` statement to synchronise the `courses` table with the information sourced from the temporary view `courses_updates`. \n",
    "\n",
    "In this scenario, we exclusively focus on the condition where there is no match. This implies that we will only insert new data if it does not already exist in our target table, based on the unique key comprising both the `course_id` and the `title` fields.\n",
    "\n",
    "Among the new courses, our interest lies only in inserting those categorised under _computer science_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebb88149-bc26-4840-9422-0474c4fdf5a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MERGE INTO courses c\n",
    "USING courses_updates u\n",
    "ON c.course_id = u.course_id AND c.title = u.title \n",
    "WHEN NOT MATCHED AND u.category = 'Computer Science' THEN INSERT * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38e6b29d-2ca3-4188-92fa-065dbf351ef6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The query execution resulted in the insertion of three new records, all belonging to the computer science category. The operation is called an **insert-only merge**, which demonstrates the merge operation's ability to prevent duplicate records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fee31c15-c870-4d76-8c70-fe15521537e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MERGE INTO courses c\n",
    "USING courses_updates u\n",
    "ON c.course_id = u.course_id AND c.title = u.title \n",
    "WHEN NOT MATCHED AND u.category = 'Computer Science' THEN INSERT * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4797612b-32e9-412d-b45e-f36b6229e418",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The second execution of our merge statement didn't lead into the reinsertion of the records, as they already exist in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c9cf517-fd3e-466a-804c-cdabbad80e71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7501389048729702,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Module 2: Transforming Data with Apache Spark",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
