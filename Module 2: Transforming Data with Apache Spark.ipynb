{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5095d7f-0553-4857-a21d-8831f3066737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Objectives\n",
    "- Querying data files\n",
    "- Writing to tables\n",
    "- Performing advanced ETL operations\n",
    "- Discover the potential of higher-order functions and user-defined functions (UDFs) in Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d49d8dba-0b0b-4182-a601-4d62034e8564",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Querying Data Files\n",
    "To initiate a file query, we use the SELECT * FROM syntax, followed by the file format and the path to the file. \n",
    "```sql\n",
    "SELECT * FROM file_format.`/path/to/file`\n",
    "```\n",
    "The filepath is specified between **backticks**, to prevent potential syntax errors and ensure the correct interpretation of the path. \n",
    "\n",
    "A filepath in this context can refer to \n",
    "- A single file\n",
    "- A wildcard character to simultaneously read multiple files; or\n",
    "- An entire directory, assuming that all files within that directory adhere to the same format and schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70716425-5a09-4a5a-bde9-5ecfcc8a7b11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can now demonstrate extracting data directly from files using a real-world dataset representing an online school environment. This dataset consists of three tables:\n",
    "- Students\n",
    "- Enrollments\n",
    "- Courses\n",
    "\n",
    "We begin by running a helper notebook, \"School-Setup\", which can be found within the `Include` subfolder. This helper notebook facilitates downloading of the dataset to the Databricks file system and prepares the working environment accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3ad5aa8-87a8-4478-aeaa-12dd3bcfb70b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/School-Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd4963b2-92be-490e-abda-9bb2a3659048",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Querying JSON Format\n",
    "The student data in this dataset is formatted in JSON. The placeholder `dataset_school` referenced in the following query, is a variable defined within our \"School-Setup\" notebook. It points to the location where the dataset files are stored on the filesystem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7f7e449-c289-4a01-8d7e-71399a4a0ef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "files = dbutils.fs.ls(f\"{dataset_school}/students-json\")\n",
    "display(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ab2f114-9485-4c34-b012-8ce2b54062b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The output above shows that there are 6 JSON files in the `students-json` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "294bb48e-d462-44b3-b719-9a5840f14132",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Reading a single data file\n",
    "To read a single JSON file, the `SELECT` statement is used with the syntax `SELECT * FROM json.`, and then the full path for the JSON file is specified between backticks. We use the `dataset.school` placeholder with the `$` character to reference the location where the dataset files are stored. This placeholder is configured in the \"School-Setup\" notebook: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7178ed28-6d89-48ce-8c28-29a42e1ece76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM json.`${dataset.school}/students-json/export_001.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8231bd02-93d7-489b-8ff5-209a4bdab825",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The result displays the extracted student data, including:\n",
    "- Student ID\n",
    "- Email\n",
    "- GPA score\n",
    "- Profile information (in JSON format); and\n",
    "- The last updated timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a618db2-6264-4e55-9df5-4bc4965d882b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Querying multiple files\n",
    "To query multiple files simultaneously, you can use the wildcard character (*) in the path. For example, you can easily query all JSON files starting with the name `export_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "723bcc94-43ee-483d-b5d9-c1232a7b1bb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM json.`${dataset.school}/students-json/export_*.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b6e04e7-ce39-4de3-ac6a-597e37a77c3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Querying an entire directory\n",
    "You can query and entire directory of files, assuming a consistent format and schema across all files in the directory. In the following query, the directory path is specified instead of an individual file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cfe8838-6235-42b3-b65d-48fada578f26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM json.`${dataset.school}/students-json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a470a23a-5988-4075-ad82-f70e601562f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Recording the source file\n",
    "When dealing with multiple files, adding the `input_file_name` function becomes useful. This built-in Spark SQL function records the source data file for each record. This helps in troubleshooting data-related issues by precisely pinpointing their exact source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c57b866c-779b-44a3-b502-88ed98c881bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT *, input_file_name() source_file FROM json.`${dataset.school}/students-json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad53c675-d5ed-42e8-b9c3-b8a9a6aaaf44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The output above shows in addition to the original columns, a new column `source_file`. This column provides supplementary information about the origin of each record in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2866eb18-423c-4ba3-83ac-1ffb27a53d6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Querying Using the text Format\n",
    "When dealing with a variety of text-based files, including formats such as JSON, CSV, TSV, and TXT, Databricks provides the flexibility to handle them using the text format:\n",
    "```sql\n",
    "SELECT * FROM text.`/path/to/file`\n",
    "```\n",
    "This format allows you to extract the data as raw strings, which provide significant advantages in scenarios where input data might be corrupted or contain anomalies. \n",
    "By extracting data as raw strings, you can leverage custom parsing logic to navigate and extract relevant values from the text-based files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "007532c3-9c53-4bc7-afea-309478db7498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM text.`${dataset.school}/students-json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "826dc63b-a4f4-41d2-918e-805707117d98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The output above displays the student data as raw strings. Each line of the file is loaded as a record within a one-string column, `named` value.\n",
    "\n",
    "With this result, you can easily apply custom parsing or transformationt techniques to extract specific fields, correct anomalies, or reformat the data as needed, for subsequent analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fa54070-e0ce-471e-beab-d45d0b092d47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Querying Using binaryFile Format\n",
    "There are scenarios where the binary representation of file content is essential, such as when working with images or unstructured data. In such cases, the `binaryFile` format is suited for this task:\n",
    "```sql\n",
    "SELECT * FROM binaryFile.`/path/sample_image.png`\n",
    "```\n",
    "In the sample query provided, the `binaryFile` format is employed to query an image (`sample_image.png`), allowing you to work directly with the binary representation of the file's content.\n",
    "\n",
    "We can use the `binaryFile` format to extract the raw bytes and some metadata information of the student files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14a17ca4-2794-48a9-b338-880245d0b985",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM binaryFile.`${dataset.school}/students-json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a21d3b4-e061-4005-9fed-c439fe55b865",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The output of the query provides the following details about each source file:\n",
    "- `path` provides the location of the source file on the storage\n",
    "- `modificationTime` gives the last modification time of the file\n",
    "- `length` indicates the size of the file\n",
    "- `content` represents the binary representation of the file\n",
    "\n",
    "So, by using the binaryFile format, you can access both the content and metadata for files, offering a detailed view of your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ade41fb7-4a7c-4716-aa81-0454103044d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Querying Non-Self-Describing Formats\n",
    "The previous querying approach is particularly effective with self-describing formats that possess a well-defined schema, such as JSON and Parquet. By nature, these formats offer a built-in structure that makes it easy to retrieve and interpret data using `SELECT` queries.\n",
    "\n",
    "However, when dealing with non-self-describing formats such as CSV, the `SELECT` statement may not be as informative. Unlike JSON and Parquet, CSV files lack a predefined schema, making the format less suitable for direct querying. In such cases, additional steps, such as defining a schema, may be necessary for effective data extraction and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e580f761-fe80-4ab6-9e48-dfe39c1f2adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM csv.`${dataset.school}/courses-csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b84e0fb9-f152-4daf-a9ba-2f1260f1bfed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As shown from the output above, the query is not well-parsed:\n",
    "- The header row is extracted as a table row; and\n",
    "- All columns are loaded into a single column, `_c0`.\n",
    "This behaviour is explained by the delimiter - the symbol used to separate columns in the file - which, in this case, is a semicolon rather than the standard comma.\n",
    "\n",
    "This issue highlights a challenge with querying files without a well-defined schema, particulary in formats like CSV. In the upcoming sections, we will learn how to address this challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8dceb19-1dff-4d2c-b770-5cc6dfea3052",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7529856598796892,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Module 2: Transforming Data with Apache Spark",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
