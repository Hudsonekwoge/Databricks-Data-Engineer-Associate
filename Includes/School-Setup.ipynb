{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9281b34-5eee-4657-9ef9-66732bdc841b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG hive_metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaff7395-6564-4eee-8ba3-9927bde70e6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_source_uri = \"s3://dalhussein-books/DEA-Book/datasets/school/v1/\"\n",
    "dataset_school = 'dbfs:/mnt/DE-Associate-Book/datasets/school'\n",
    "checkpoint_path = 'dbfs:/mnt/DEA-Book/checkpoints'\n",
    "dlt_path = 'dbfs:/mnt/DEA-Book/dlt'\n",
    "db_name = 'DE_Associate_School'\n",
    "dlt_db_name = 'school_dlt_db'\n",
    "spark.conf.set(f\"dataset.school\", dataset_school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d75896e3-503f-49bd-95ca-1c9b5f41947d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_up():\n",
    "    print(\"Removing Checkpoints ...\")\n",
    "    dbutils.fs.rm(checkpoint_path, True)\n",
    "    print(\"Removing DLT storage location ...\")\n",
    "    dbutils.fs.rm(dlt_path, True)\n",
    "    print(\"Dropping Database ...\")\n",
    "    spark.sql(f\"DROP SCHEMA IF EXISTS {db_name} CASCADE\")\n",
    "    print(\"Dropping DLT database ...\")\n",
    "    spark.sql(f\"DROP SCHEMA IF EXISTS {dlt_db_name} CASCADE\")\n",
    "    print(\"Removing Dataset ...\")\n",
    "    dbutils.fs.rm(dataset_school, True)\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0ce23c0-c54f-4b85-811e-810f22b537ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    clean = int(dbutils.widgets.get(\"clean\"))\n",
    "except:\n",
    "    clean = 0\n",
    "\n",
    "if clean:\n",
    "    clean_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c2872ad-7e7f-43dd-9464-711087489884",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def path_exists(path):\n",
    "  try:\n",
    "    dbutils.fs.ls(path)\n",
    "    return True\n",
    "  except Exception as e:\n",
    "    if 'java.io.FileNotFoundException' in str(e):\n",
    "      return False\n",
    "    else:\n",
    "      raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b762c090-b529-4923-8566-f4a318291c9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def download_dataset(source, target):\n",
    "    files = dbutils.fs.ls(source)\n",
    "\n",
    "    for f in files:\n",
    "        source_path = f\"{source}/{f.name}\"\n",
    "        target_path = f\"{target}/{f.name}\"\n",
    "        if not path_exists(target_path):\n",
    "            print(f\"Copying {f.name} ...\")\n",
    "            dbutils.fs.cp(source_path, target_path, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e95c1f92-7333-48bd-971b-a2ec34dc0403",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_index(dir):\n",
    "    files = dbutils.fs.ls(dir)\n",
    "    index = 0\n",
    "    if files:\n",
    "        file = max(files).name\n",
    "        index = int(file.rsplit('.', maxsplit=1)[0])\n",
    "    return index+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fe6e86b-2555-463a-8c83-7009f7e35b15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Structured Streaming\n",
    "streaming_dir = f\"{dataset_school}/enrollments-json-streaming\"\n",
    "raw_dir = f\"{dataset_school}/enrollments-json-raw\"\n",
    "\n",
    "def load_file(current_index):\n",
    "    latest_file = f\"{str(current_index).zfill(2)}.json\"\n",
    "    print(f\"Loading {latest_file} file to the school dataset\")\n",
    "    dbutils.fs.cp(f\"{streaming_dir}/{latest_file}\", f\"{raw_dir}/{latest_file}\")\n",
    "\n",
    "    \n",
    "def load_new_data(all=False):\n",
    "    index = get_index(raw_dir)\n",
    "    if index >= 10:\n",
    "        print(\"No more data to load\\n\")\n",
    "\n",
    "    elif all == True:\n",
    "        while index <= 10:\n",
    "            load_file(index)\n",
    "            index += 1\n",
    "    else:\n",
    "        load_file(index)\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28edcd0f-3f61-43b1-9a71-03159829555d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# DLT\n",
    "streaming_enrollments_dir = f\"{dataset_school}/enrollments-dlt-streaming\"\n",
    "streaming_courses_dir = f\"{dataset_school}/courses-streaming\"\n",
    "\n",
    "raw_enrollments_dir = f\"{dataset_school}/enrollments-dlt-raw\"\n",
    "raw_courses_dir = f\"{dataset_school}/courses-cdc\"\n",
    "\n",
    "def load_json_file(current_index):\n",
    "    latest_file = f\"{str(current_index).zfill(2)}.json\"\n",
    "    print(f\"Loading {latest_file} enrollments file to the school dataset\")\n",
    "    dbutils.fs.cp(f\"{streaming_enrollments_dir}/{latest_file}\", f\"{raw_enrollments_dir}/{latest_file}\")\n",
    "    print(f\"Loading {latest_file} courses file to the school dataset\")\n",
    "    dbutils.fs.cp(f\"{streaming_courses_dir}/{latest_file}\", f\"{raw_courses_dir}/{latest_file}\")\n",
    "\n",
    "    \n",
    "def load_new_json_data(all=False):\n",
    "    index = get_index(raw_enrollments_dir)\n",
    "    if index >= 10:\n",
    "        print(\"No more data to load\\n\")\n",
    "\n",
    "    elif all == True:\n",
    "        while index <= 10:\n",
    "            load_json_file(index)\n",
    "            index += 1\n",
    "    else:\n",
    "        load_json_file(index)\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "425c3302-5c7f-4a21-9ced-b989f39531be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#clean_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f946fbd3-8508-4baf-a207-6178651aee30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "download_dataset(data_source_uri, dataset_school)\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {db_name}\")\n",
    "spark.sql(f\"USE {db_name}\")\n",
    "print()\n",
    "print(f\"Schema name: hive_metastore.{db_name}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "School-Setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
